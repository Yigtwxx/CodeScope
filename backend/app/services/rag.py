from typing import AsyncIterable
from langchain_community.llms import Ollama
from langchain_core.prompts import PromptTemplate
from app.db.chroma import get_vector_store
from app.core.config import settings
from app.services.hybrid_search import hybrid_search

def get_llm():
    return Ollama(
        base_url=settings.OLLAMA_BASE_URL,
        model=settings.OLLAMA_MODEL
    )

async def chat_stream(query: str) -> AsyncIterable[str]:
    """
    Generates a streaming response for the given query using RAG.
    """
    vector_store = get_vector_store()
    llm = get_llm()

    # Prompt template for RAG responses
    template = """You are CodeScope, an intelligent bilingual coding assistant.

Use the following codebase context to answer the user's question.

**CRITICAL LANGUAGE RULE - READ CAREFULLY:**
1. DETECT the question language FIRST
2. Use ONLY that language for the ENTIRE response
3. If question is in Turkish â†’ ENTIRE answer in Turkish (no English mixing!)
4. If question is in English â†’ ENTIRE answer in English (no Turkish mixing!)
5. NEVER switch languages mid-response
6. ALL technical terms, code explanations, examples MUST be in the detected language

**Examples:**
- Turkish question: "Bu kod ne yapar?" â†’ Answer fully in Turkish
- English question: "What does this code do?" â†’ Answer fully in English

**Response Structure:**
- Start with a summary in detected language
- Provide detailed explanation in detected language
- Add code examples with comments in detected language
- Use proper formatting (headings, code blocks, lists)

Context from codebase:
{context}

Question:
{question}

Your detailed answer (in the SAME language as question, NO language mixing):
"""
    
    prompt = PromptTemplate(
        template=template,
        input_variables=["context", "question"]
    )

    # Retrieve relevant documents (top 8) using hybrid search
    try:
        # Try hybrid search first (semantic + BM25)
        try:
            docs = hybrid_search(query, vector_store, k=8)
            print(f"ğŸ“Š Hybrid Search Retrieved: {len(docs)} chunks")
        except Exception as hybrid_err:
            # Fallback to semantic-only if hybrid fails
            print(f"âš ï¸  Hybrid search failed, using semantic-only: {hybrid_err}")
            docs = vector_store.similarity_search(query, k=8)
            print(f"ğŸ“Š Semantic Search Retrieved: {len(docs)} chunks")
        
        if len(docs) == 0:
            print("âš ï¸  WARNING: No relevant chunks found!")
            print("â”€"*60 + "\n")
            yield "âš ï¸ **ChromaDB boÅŸ! LÃ¼tfen Settings'den repo'yu aÃ§Ä±n (Open Repository).**\n\n"
            yield "Repository aÃ§tÄ±ktan sonra:\n"
            yield "1. Backend terminal'de 'ğŸ‰ INGESTION COMPLETE!' mesajÄ±nÄ± bekleyin\n"
            yield "2. Daha sonra sorularÄ±nÄ±zÄ± sorun\n"
            return
            
        print(f"ğŸ“š Using {len(docs)} code chunks as context")
        print("â”€"*60 + "\n")
        
        # Show sources BEFORE generating answer (like ChatGPT/Gemini)
        yield "\n **AraÅŸtÄ±rÄ±lan Dosyalar:**\n\n"
        
        unique_sources = {}
        for doc in docs:
            filename = doc.metadata.get('filename', 'Unknown')
            rel_path = doc.metadata.get('relative_path', filename)
            language = doc.metadata.get('language', 'unknown')
            
            if rel_path not in unique_sources:
                unique_sources[rel_path] = {
                    'filename': filename,
                    'language': language,
                    'path': rel_path
                }
        
        for idx, (path, info) in enumerate(unique_sources.items(), 1):
            yield f"{idx}. ğŸ“„ `{info['filename']}` ({info['language']})\n"
            yield f"   â””â”€ {path}\n"
        
        yield "\nğŸ’­ **Cevap hazÄ±rlanÄ±yor...**\n\n"
        
    except Exception as e:
        print(f"âŒ ERROR during retrieval: {e}")
        print("â”€"*60 + "\n")
        yield f"âŒ **Arama HatasÄ±:** {str(e)}\n\nLÃ¼tfen Settings'den repo'yu tekrar aÃ§Ä±n."
        return
    
    # Prepare context from retrieved documents
    context = "\n\n".join([doc.page_content for doc in docs])
    
    # Create the RAG chain
    chain = prompt | llm
    
    # Stream the response
    try:
        for chunk in chain.stream({"context": context, "question": query}):
            yield chunk
    except Exception as e:
        error_msg = str(e)
        if "Cannot connect to host" in error_msg or "Connection refused" in error_msg:
            yield "ğŸ”´ **Error: Ollama is not running.**\n\n"
            yield "To use CodeScope, you need a local LLM running via Ollama.\n"
            yield "1. Download Ollama from [ollama.com](https://ollama.com).\n"
            yield "2. Install and run it.\n"
            yield f"3. Pull the model: `ollama pull {settings.OLLAMA_MODEL}`\n"
            yield "4. Restart CodeScope."
        else:
            yield f"ğŸ”´ **An error occurred:** {error_msg}"

