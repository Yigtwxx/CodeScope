from typing import AsyncIterable
from langchain_community.llms import Ollama
from langchain_core.prompts import PromptTemplate
from app.db.chroma import get_vector_store
from app.core.config import settings
from app.services.hybrid_search import hybrid_search

def get_llm():
    """Ollama LLM Ã¶rneÄŸini dÃ¶ndÃ¼rÃ¼r."""
    return Ollama(
        base_url=settings.OLLAMA_BASE_URL,
        model=settings.OLLAMA_MODEL
    )

async def chat_stream(query: str) -> AsyncIterable[str]:
    """
    RAG (Retrieval-Augmented Generation) kullanarak sorgu iÃ§in canlÄ± (streaming) bir cevap Ã¼retir.
    """
    vector_store = get_vector_store()
    llm = get_llm()

    # RAG cevaplarÄ± iÃ§in ÅŸablon (Prompt Template)
    # Modelin aynÄ± dilde cevap vermesini zorlayan kurallar iÃ§erir
    template = """You are CodeScope, an intelligent bilingual coding assistant.

Use the following codebase context to answer the user's question.

**CRITICAL LANGUAGE RULE - READ CAREFULLY:**
1. DETECT the question language FIRST
2. Use ONLY that language for the ENTIRE response
3. If question is in Turkish â†’ ENTIRE answer in Turkish (no English mixing!)
4. If question is in English â†’ ENTIRE answer in English (no Turkish mixing!)
5. NEVER switch languages mid-response
6. ALL technical terms, code explanations, examples MUST be in the detected language

**Examples:**
- Turkish question: "Bu kod ne yapar?" â†’ Answer fully in Turkish
- English question: "What does this code do?" â†’ Answer fully in English

**Response Structure:**
- Start with a summary in detected language
- Provide detailed explanation in detected language
- Add code examples with comments in detected language
- Use proper formatting (headings, code blocks, lists)

Context from codebase:
{context}

Question:
{question}

Your detailed answer (in the SAME language as question, NO language mixing):
"""
    
    prompt = PromptTemplate(
        template=template,
        input_variables=["context", "question"]
    )

    # Ã–ncelikle ChromaDB'de herhangi bir belge olup olmadÄ±ÄŸÄ±nÄ± kontrol et
    try:
        all_docs = vector_store.get()
        total_docs = len(all_docs.get('ids', []))
        
        if total_docs == 0:
            print("âš ï¸  WARNING: ChromaDB is empty! No repository loaded.")
            print("â”€"*60 + "\n")
            yield "âš ï¸ **HenÃ¼z bir repository aÃ§Ä±lmadÄ±!**\n\n"
            yield "ğŸ“ **LÃ¼tfen ÅŸu adÄ±mlarÄ± takip edin:**\n\n"
            yield "1. SaÄŸ Ã¼stteki **âš™ï¸ Settings** butonuna tÄ±klayÄ±n\n"
            yield "2. **'Open Repository'** butonuna tÄ±klayÄ±n\n"
            yield "3. Analiz etmek istediÄŸiniz kod klasÃ¶rÃ¼nÃ¼ seÃ§in\n"
            yield "4. Backend terminal'de **'ğŸ‰ INGESTION COMPLETE!'** mesajÄ±nÄ± bekleyin\n"
            yield "5. TamamlandÄ±ktan sonra sorularÄ±nÄ±zÄ± sorun! ğŸš€\n"
            return
        
        print(f"âœ“ ChromaDB ready: {total_docs} chunks available")
    except Exception as e:
        print(f"âŒ ERROR checking ChromaDB: {e}")
        print("â”€"*60 + "\n")
        yield "âŒ **VeritabanÄ± HatasÄ±!**\n\n"
        yield "LÃ¼tfen Settings'den repository aÃ§mayÄ± deneyin.\n"
        return

    # Ä°lgili belgeleri (ilk 8 parÃ§a) hibrit arama ile getir
    try:
        # Ã–nce hibrit aramayÄ± dene (anlamsal + BM25)
        try:
            docs = hybrid_search(query, vector_store, k=8)
            print(f"ğŸ“Š Hybrid Search Retrieved: {len(docs)} chunks")
        except Exception as hybrid_err:
            # Hibrit baÅŸarÄ±sÄ±z olursa sadece anlamsal aramaya dÃ¶n
            print(f"âš ï¸  Hybrid search failed, using semantic-only: {hybrid_err}")
            docs = vector_store.similarity_search(query, k=8)
            print(f"ğŸ“Š Semantic Search Retrieved: {len(docs)} chunks")
        
        if len(docs) == 0:
            print("âš ï¸  WARNING: No relevant chunks found!")
            print("â”€"*60 + "\n")
            yield "âš ï¸ **Bu sorguyla ilgili sonuÃ§ bulunamadÄ±!**\n\n"
            yield "FarklÄ± bir soru sormayÄ± deneyin veya repository'nizin doÄŸru yÃ¼klendiÄŸinden emin olun.\n"
            return
            
        print(f"ğŸ“š Using {len(docs)} code chunks as context")
        print("â”€"*60 + "\n")
        
        # CevabÄ± Ã¼retmeden Ã¶nce kullanÄ±lan kaynak dosyalarÄ± gÃ¶ster
        yield "\nğŸ“š **AraÅŸtÄ±rÄ±lan Dosyalar:**\n\n"
        
        unique_sources = {}
        for doc in docs:
            filename = doc.metadata.get('filename', 'Unknown')
            rel_path = doc.metadata.get('relative_path', filename)
            language = doc.metadata.get('language', 'unknown')
            
            if rel_path not in unique_sources:
                unique_sources[rel_path] = {
                    'filename': filename,
                    'language': language,
                    'path': rel_path
                }
        
        for idx, (path, info) in enumerate(unique_sources.items(), 1):
            yield f"{idx}. ğŸ“„ `{info['filename']}` ({info['language']})\n"
            yield f"   â””â”€ {path}\n"
        
        yield "\nğŸ’­ **Cevap hazÄ±rlanÄ±yor...**\n\n"
        
    except Exception as e:
        print(f"âŒ ERROR during retrieval: {e}")
        print("â”€"*60 + "\n")
        yield f"âŒ **Arama HatasÄ±:** {str(e)}\n\nLÃ¼tfen Settings'den repo'yu tekrar aÃ§Ä±n."
        return
    
    # Getirilen belgelerden iÃ§eriÄŸi hazÄ±rla
    context = "\n\n".join([doc.page_content for doc in docs])
    
    # RAG zincirini (chain) oluÅŸtur
    chain = prompt | llm
    
    # CevabÄ± stream (parÃ§a parÃ§a) olarak dÃ¶ndÃ¼r
    try:
        for chunk in chain.stream({"context": context, "question": query}):
            yield chunk
    except Exception as e:
        error_msg = str(e)
        if "Cannot connect to host" in error_msg or "Connection refused" in error_msg:
            yield "ğŸ”´ **Error: Ollama is not running.**\n\n"
            yield "To use CodeScope, you need a local LLM running via Ollama.\n"
            yield "1. Download Ollama from [ollama.com](https://ollama.com).\n"
            yield "2. Install and run it.\n"
            yield f"3. Pull the model: `ollama pull {settings.OLLAMA_MODEL}`\n"
            yield "4. Restart CodeScope."
        else:
            yield f"ğŸ”´ **An error occurred:** {error_msg}"


