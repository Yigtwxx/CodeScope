import os
import sys

# Windows konsolu iÃ§in UTF-8 Ã§Ä±ktÄ±sÄ±nÄ± garantiye al
if sys.stdout.encoding != 'utf-8':
    sys.stdout.reconfigure(encoding='utf-8')

from typing import List
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter, Language
from langchain_core.documents import Document
from app.db.chroma import get_vector_store

# Kod dosyalarÄ± iÃ§in desteklenen uzantÄ±lar
SUPPORTED_EXTENSIONS = {
    ".py", ".js", ".ts", ".tsx", ".jsx", ".md", ".txt", 
    ".java", ".go", ".cpp", ".c", ".h", ".cs", ".php", ".rb", ".rs", ".swift", ".kt"
}

# GÃ¶z ardÄ± edilecek dizinler
IGNORED_DIRS = {
    ".git", "node_modules", "__pycache__", "venv", "env", ".idea", ".vscode", "dist", "build", "coverage"
}

# Toplu iÅŸlem yapÄ±landÄ±rmasÄ±
BATCH_SIZE = 166  # ChromaDB iÃ§in gÃ¼venli toplu iÅŸlem boyutu (sÄ±nÄ±r ~5000)
BATCH_DELETE_LIMIT = 5000  # Tek seferde silinecek maksimum ID sayÄ±sÄ±

def is_valid_file(file_path: str) -> bool:
    ext = os.path.splitext(file_path)[1]
    return ext in SUPPORTED_EXTENSIONS

def detect_language(extension: str) -> str:
    """Dosya uzantÄ±sÄ±ndan programlama dilini tespit eder"""
    lang_map = {
        ".py": "python",
        ".js": "javascript",
        ".jsx": "javascript",
        ".ts": "typescript",
        ".tsx": "typescript",
        ".java": "java",
        ".go": "go",
        ".cpp": "cpp",
        ".c": "c",
        ".h": "c",
        ".cs": "csharp",
        ".php": "php",
        ".rb": "ruby",
        ".rs": "rust",
        ".swift": "swift",
        ".kt": "kotlin",
        ".md": "markdown",
        ".txt": "text",
    }
    return lang_map.get(extension, "unknown")

def get_code_aware_splitter(extension: str) -> RecursiveCharacterTextSplitter:
    """Daha iyi parÃ§alama (chunking) iÃ§in dile Ã¶zel kod ayÄ±rÄ±cÄ±sÄ±nÄ± getirir"""
    lang_splitter_map = {
        ".py": Language.PYTHON,
        ".js": Language.JS,
        ".jsx": Language.JS,
        ".ts": Language.TS,
        ".tsx": Language.TS,
        ".java": Language.JAVA,
        ".cpp": Language.CPP,
        ".c": Language.CPP,
        ".go": Language.GO,
        ".rs": Language.RUST,
        ".md": Language.MARKDOWN,
    }
    
    try:
        if extension in lang_splitter_map:
            return RecursiveCharacterTextSplitter.from_language(
                language=lang_splitter_map[extension],
                chunk_size=1000,
                chunk_overlap=200,
            )
    except Exception as e:
        print(f"âš ï¸  Language splitter failed for {extension}: {e}")
    
    # Genel ayÄ±rÄ±cÄ±ya (fallback) dÃ¶n
    return RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
    )

def load_documents(repo_path: str) -> List[Document]:
    documents = []
    for root, dirs, files in os.walk(repo_path):
        # YoksayÄ±lan dizinleri atlamak iÃ§in dirs listesini yerinde deÄŸiÅŸtir
        dirs[:] = [d for d in dirs if d not in IGNORED_DIRS]
        
        for file in files:
            file_path = os.path.join(root, file)
            if is_valid_file(file_path):
                try:
                    ext = os.path.splitext(file_path)[1]
                    # DosyayÄ± UTF-8 olarak yÃ¼kle, kodlamayÄ± otomatik algÄ±la
                    loader = TextLoader(file_path, encoding="utf-8", autodetect_encoding=True)
                    docs = loader.load()
                    
                    # ZenginleÅŸtirilmiÅŸ metadata ekle
                    for doc in docs:
                        doc.metadata["source"] = file_path
                        doc.metadata["filename"] = file
                        doc.metadata["extension"] = ext
                        doc.metadata["language"] = detect_language(ext)
                        doc.metadata["relative_path"] = os.path.relpath(file_path, repo_path)
                    
                    documents.extend(docs)
                except Exception as e:
                    print(f"Error loading file {file_path}: {e}")
                    continue
    return documents

def ingest_repository_stream(repo_path: str):
    """
    Bir depoyu (repository) iÅŸler ve ilerleme durumunu parÃ§a parÃ§a (yield) dÃ¶ndÃ¼rÃ¼r.
    """
    yield "ğŸ“¦ Starting repository ingestion...\n"
    print("\n" + "="*60)
    print("ğŸš€ STARTING REPOSITORY INGESTION")
    print("="*60)
    
    if not os.path.exists(repo_path):
        error_msg = f"âŒ ERROR: Repository path not found: {repo_path}"
        print(error_msg)
        yield error_msg + "\n"
        return
    
    print(f"ğŸ“‚ Repository: {repo_path}")
    yield f"ğŸ“‚ Repository: {repo_path}\n"

    # 1. Belgeleri YÃ¼kle
    step_msg = "\nğŸ“– STEP 1/3: Loading documents..."
    print(step_msg)
    yield step_msg + "\n"
    
    raw_documents = load_documents(repo_path)
    if not raw_documents:
        error_msg = "âš ï¸  No valid documents found"
        print(error_msg)
        yield error_msg + "\n"
        return
    
    success_msg = f"âœ… Loaded {len(raw_documents)} files"
    print(success_msg)
    yield success_msg + "\n"

    # 2. Metni Koda DuyarlÄ± ParÃ§ala (Code-Aware Chunking)
    step_msg = "\nâœ‚ï¸  STEP 2/3: Code-aware chunking..."
    print(step_msg)
    yield step_msg + "\n"
    chunks = []
    
    # Verimli iÅŸleme iÃ§in belgeleri uzantÄ±larÄ±na gÃ¶re grupla
    from collections import defaultdict
    docs_by_ext = defaultdict(list)
    for doc in raw_documents:
        ext = doc.metadata.get("extension", ".txt")
        docs_by_ext[ext].append(doc)
    
    # Her uzantÄ± grubunu uygun ayÄ±rÄ±cÄ± ile iÅŸle
    for ext, docs in docs_by_ext.items():
        splitter = get_code_aware_splitter(ext)
        lang = detect_language(ext)
        ext_chunks = splitter.split_documents(docs)
        chunks.extend(ext_chunks)
        print(f"   ğŸ’ {lang.capitalize():12} â†’ {len(ext_chunks):4} chunks from {len(docs):3} files")
    
    total_msg = f"âœ… Total chunks: {len(chunks)}"
    print(total_msg)
    yield total_msg + "\n"
    
    # 2.5. Kod ZekasÄ±nÄ± Ã‡Ä±kar (AST Parsing)
    try:
        from app.services.code_intelligence import extract_code_entities, add_entities_to_metadata
        print("\nğŸ§  Extracting code intelligence (functions, classes)...")
        yield "\nğŸ§  Extracting code intelligence...\n"
        
        entities_by_file = extract_code_entities(raw_documents)
        if entities_by_file:
            chunks = add_entities_to_metadata(chunks, entities_by_file)
            yield "âœ… Code intelligence extracted\n"
    except Exception as e:
        print(f"âš ï¸  Code intelligence failed (non-critical): {e}")
        yield f"âš ï¸  Code intelligence skipped: {str(e)}\n"

    # 3. VektÃ¶r VeritabanÄ±nda Sakla (ChromaDB)
    step_msg = "\nğŸ’¾ STEP 3/3: Storing in ChromaDB..."
    print(step_msg)
    yield step_msg + "\n"
    vector_store = get_vector_store()
    
    # Depo deÄŸiÅŸikliÄŸini desteklemek iÃ§in mevcut belgeleri temizle
    print("ğŸ§¹ Clearing old repository data...")
    try:
        existing_docs = vector_store.get()
        ids_to_delete = existing_docs.get('ids', [])
        
        if ids_to_delete:
            print(f"   ğŸ—‘ï¸  Deleting {len(ids_to_delete)} existing chunks...")
            # BÃ¼yÃ¼k veri kÃ¼meleri iÃ§in partiler halinde sil
            for i in range(0, len(ids_to_delete), BATCH_DELETE_LIMIT):
                batch_ids = ids_to_delete[i : i + BATCH_DELETE_LIMIT]
                vector_store.delete(batch_ids)
            print("   âœ… Old data cleared")
        else:
            print("   â„¹ï¸  No existing data (fresh database)")
    except Exception as e:
        print(f"   âš ï¸  Cleanup warning: {e}")

    # Belgeleri partiler halinde depoya ekle
    print(f"ğŸ“¥ Adding {len(chunks)} chunks to vector store...")
    try:
        if chunks:
            total_chunks = len(chunks)
            batch_count = (total_chunks + BATCH_SIZE - 1) // BATCH_SIZE
            for i in range(0, total_chunks, BATCH_SIZE):
                batch = chunks[i : i + BATCH_SIZE]
                batch_num = i // BATCH_SIZE + 1
                print(f"   ğŸ“¦ Batch {batch_num}/{batch_count}: {len(batch)} chunks")
                vector_store.add_documents(batch)
            print("   âœ… All chunks stored successfully")
        else:
            print("   âš ï¸  No chunks to add")
    except Exception as e:
        print(f"   âŒ Failed to add documents: {e}")
        raise

    complete_msg = "\n" + "="*60 + "\nğŸ‰ INGESTION COMPLETE!\n" + f"   ğŸ“ Files: {len(raw_documents)}\n" + f"   ğŸ§© Chunks: {len(chunks)}\n" + "="*60
    print(complete_msg)
    yield complete_msg + "\n"

